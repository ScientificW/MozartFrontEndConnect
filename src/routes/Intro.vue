<template>
  <div class="row">
    <div class="side">
      <div class="col-3 menu">
        <ul>
          <li><a href="#VOCALOID_"> 项目简介</a></li>
          <li><a href="#1">前期探索</a></li>
          <li><a href="#2">第一部分</a></li>
          <li><a href="#3">第二部分</a></li>
          <li><a href="#4">项目后期</a></li>
        </ul>
        <!-- 定义一个 HTML 无序列表 -->
      </div>
    </div>
    <div class="main">
      <!-- 第一部分 -->
      <a id="VOCALOID_">
        <h2>“点彩成乐” 简介</h2>
      </a>
      <p>
        当今社会，人们趋向于在线上分享自己的生活，而其中抖音短视频以及图片拼贴画广受用户喜爱。然而如何选择合适的音乐去适配画面，以及音乐的版权问题引起了用户及企业的担忧。“点彩成乐”项目致力于用AI深度学习的方式去解决图像/视频配乐问题（即使用合理的机器学习算法对视频影像或图片的内容进行分析，并创作出与之相匹配的音乐），让每个人都有为自己的作品（图片/视频）进行针对性配乐的机会，成为独一无二的创作者。
      </p>
      <br /><br /><br />
      <!-- 第一部分 -->
      <a id="1">
        <h2>前期探索</h2>
      </a>
      <p>
        项目前期以学习为主，小组成员分别展开了对于机器学习及深度学习算法的探索，进行了相关论文的查询与阅读。在项目前期过程中，每个小组成员分工合作，负责探索一种机器学习或深度学习算法，深入研究其理论基础、算法原理和实现方式，并将所得到的知识进行分享和讨论，并逐步提高小组整体对于论文的理解程度。
      </p>
      <p>
        对于机器学习算法，我们将通对于各类人工智能算法的原理进行学习，来决定我们将要在项目中使用的算法。同时我们尝试对算法进行相当程度的改进，让我们的项目能够更加智能的实现目标。在我们的计划中，对机器学习算法合理的探究、使用和改进是我们成功的基石。
      </p>
      <p>
        要想实现对于各类人工智能和机器学习的算法进行探究，前期的对于各类算法的技术细节进行了解是必不可少的，因此我们通过论文阅读探究对各类算法的实践功能，以及在图像识别领域以及乐曲生成中不同机器学习/深度学习算法的优劣之处。
      </p>
      <p>
        在进行初步探索后，我们决定借助机器学习算法搭建项目雏形。具体来说，我们先通过一种机器学习算法，将用户输入的图片信号转化为一维矢量，并借助图片转换模型生成对应的情感分类的标记，再将该标记同图片矢量组合，借助音乐生成的机器学习算法，转换为一段MIDI音乐片段。
      </p>
      <br /><br /><br />
      <a id="2">
        <h2>第一部分</h2>
      </a>
      <p>
        对于第一阶段，我们参考了论文<b><i>
            “Studying Aesthetics in Photographic Images Using a Computational
            Approach”</i></b>，通过机器学习方法研究摄影图像中美学质量的主观概念。我们复现了论文中提到的，基于几个低级视觉特征（如色彩、对比度和纹理）量化美学质量的方法，利用计算机视觉的技术从图像中提取，并用以进一步的机器学习工作。
      </p>
      <p>
        小组使用了Emotion6数据集作为项目的训练集与测试集。该数据集共包含1982张照片，并且被平均分成六组：anger,
        disgust, fear, joy, sadness,
        surprise。在实际测试中，我们选用了其中anger, fear, joy, sadness
        四组进行模型的训练与测试。
      </p>
      <p>
        我们利用OpenCV工具，将图片转化为HSV和RGB色图，将图片矢量化之后作为新的数据集，并使用支持向量机（SVM）的机器学习算法对其进行四种情感的分类工作。SVM
        的主要思想是在特征空间中找到一个超平面，使得其能够将不同类别的数据分开，并且间隔最大。SVM模型的优化目标是最大化边缘，即支持向量到超平面的距离最大化，并且可适用于多分类操作。
      </p>
      <p>
        最终，我们成功实现了对于图片情感的分类，并取得了37%
        的准确率，相较于随机猜测的25% 准确率有了一定提升。
      </p>
      <br /><br /><br />
      <a id="3">
        <h2>第二部分</h2>
      </a>
      <p>
        对于第二阶段，我们研究了诸多人工智能生成音乐相关的论文。在论文<b><i>“This Time with Feeling: Learning Expressive Musical
            Performance”</i></b>中，作者指出可以先将乐曲MIDI片段先转化为413个经由独热编码的音符矢量，再整理为训练集交给深度学习模型（LSTM）生成音乐片段；而在另一篇论文
        <b><i>“MusicBERT: Symbolic Music Understanding with Large-Scale
            Pre-Training”</i></b>
        中，作者将音符编码视为包含8个元素的元组，这8个元素代表着音符不同方面的特征，包括时间符号、节奏、小节、位置、乐器、音高、持续时间以及曲速，再由NLP中的BERT模型进行训练。
      </p>
      <p>
        但是，由于第一阶段图像情感识别中缺乏对图片语义的信息处理，因此无法进一步提高模型识别的准确性。另外，通过简单的情感符号分类生成不同情感基调的音乐，在实现上存在缺少多样性和独特性。
      </p>
      <br /><br /><br />
      <a id="4">
        <h2>项目后期</h2>
      </a>
      <p>
        在项目后期中，我们转换了项目的实现思路：根据论文<b><i>
            “Automated Music Generation for Visual Art through Emotion”</i></b>
        所阐释的方法，我们决定通过从具有相同情感标签的图片与音乐片段中提取特征，并利用Encoder-decoder的模型框架去生成具有对应情感色彩的音乐，其中我们采用
        RNN 模型进行训练。
      </p>
      <p>
        图像训练的过程中，我们采用了https://www.imageemotion.org
        网站上提供的情感图片数据集，他将图片共分成amusement, anger, awe,
        contentment, disgust, excitement, fear, sad 共八类，我们取其中amusement,
        anger, contentment, excitement, sadness
        五类作为我们项目使用的数据集；具有不同情感的MIDI音乐数据集来自“MIREX-like
        emotion dataset”，并经由小组成员进行了手动情感分类。
      </p>
    </div>
  </div>
</template>
